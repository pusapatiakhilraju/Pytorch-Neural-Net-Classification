You're going to cover a standard PyTorch workflow (it can be chopped and changed as necessary but it covers the main outline of steps).
Process:
1. Get data read 
2. build or pick pre-trained model - > 2.1 pick loss function and optimiser - > 2.2 build training loop 
3. fit the model to data and make prediction 
4. evaluate model
5. improve through experimentation and fine tuning
6. save and reload the trained model

Building model involves: 
1. Start with random values (weights and bias)
2. Look at the training data and adjust the random values to better represent the ideal values
**NOTE** premise of deep learning is to start with random values and get close to the actual values.
 

These adjustments are made through 2 algorithms
1. Gradient Descent needs "requires_grad = True". when we run computations, this allows pytorch to keep track of parameters and update them.
2. Back Propogation

Pytorch model building essentials
1. torch.nn -> contains all of the buildings for computational graphs
2. torch.nn.Parameter -> what parameters should the models learn. 
3. torch.nn.Module - > the base class for all neural netowrk modules/
4. torch.optmis -> this is where the optimisers in Pytorch live, they will help with gradient Descent
5. def forward() -> all nn.Modulesubclasses require you to overwrite forward(). This methood defines what happens in forward computation.

Use ".parameters()" to find the list of parameters (OR) use ".state_dict()" to get the dictionary of parameters 

Predictions can be made using "torch.inference_mode()". This helps nopt to keep track of all the things like gradients. This is helpful during predictions because it makes predictions faster. Helpful while predicting large datasets.

In pytroch fine tuning is done using the loss and optimizer: The loss defines how wrong the predictions are when compared to the actual outputs. The optimizer then takes this loss to adjust the parameters to improve the loss. Loss needs to be lower. 

In training:
1. Loop through the data
2. Forward pass
3. Calculate the loss (compare the forward pass results with ground truth labels)
4. Optimize the zero grad(). zero_grad is a function in the optimizer. This is done so that the gradient is not accumulated. If gradient is accumulating then the losses on the graph will never show a decrease.
5. Loss backwards - move backwards through the network to calculate the gradients of each parameter with respect to the loss
6. optimizer step - use the optimizer to adjust the parameters and improve the loss (** Gradient Descent **)

In testing Loop:
1. set model to eval()
2. with torch.inference mode 
3. do forward pass to find the test predictions
4. calculate the loss



